<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Smart Grid Runbook</title>
    <link rel="stylesheet" href="toc.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <style>
        body{
          font-family: 'Roboto';
        }
        a {
            text-decoration: none;
        }

        p {
            line-height: 1.5
        }

        li {
            line-height:1.6
        }

    </style>
</head>
<body>

<h1 style="font-size:40px;">Smart Grid Runbook</h1>

<div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
    <li><a href="#1">1. Helpful Links</a></li>
    <li><a href="#2">2. Getting Started</a></li>
    <li><a href="#3">3. Configurations</a></li>
    <li><a href="#4">4. Input Files</a></li>
    <li><a href="#5">5. Model Pipeline Steps</a></li>
    <li><a href="#6">6. Output Files</a></li>
    <li><a href="#7">7. Expanding to a new domain</a></li>
    <li><a href="#8">8. Takeaways</a></li>
</ul>
</div>

<div id="1">
<h1>Helpful Links</h1>
<ul>
    <li><a href="https://horizon.bankof.com/scm/projects/PLATYPUS/repos/smartgrid_dltk">Git repository</a></li>
    <li><a href="http://sharepoint.bankof.com/sites/Platypus/SmartGridCS/_layouts/15/WopiFrame.aspx?sourcedoc={D8816AFB-CBDE-4F64-80AC-F073F740F828}&file=MDD%20Smartgrid%20-%20Working%20Copy.docx&action=default">Model Documentation (MDD)</a></li>
    <li><a href="http://ah-1064594-001.sdi.corp.bankof.com:8000/en-US/app/search/smart_grid__financial_center_uat">ION3000 Splunk Dashboard for UAT</a></li>
    <li><a href="http://ah-1064594-001.sdi.corp.bankof.com:8000/en-US/app/search/smart_grid__global_core_uat">Global Core Splunk Dashboard for UAT</a></li>
</ul>
</div>

<hr>

<div id="2">
<h1>Getting Started</h1>
    <ol>
    <li>Clone repository from Bitbucket at https://horizon.bankof.com/scm/projects/PLATYPUS/repos/smartgrid_dltk</li>
    <li>Install the required python packages</li>
        <ul>
            <li>pandas==1.1.5</li>
            <li>numpy==1.19.5</li>
            <li>scikit-learn==0.24.1</li>
            <li>scipy==1.5.4</li>
            <li>category_encoders==2.2.2</li>
            <li>matplotlib==3.3.3</li>
            <li>regex==2020.11.13</li>
            <li>yaml==5.3.1</li>
            <li>requests==2.25.1</li>
            <li>json==2.0.9</li>
        </ul>
    <li>Add smartgrid_dltk/src/ to PYTHONPATH</li>
    <li>Run <b>get_training_data.py</b></li>
    </ol>

    You now should have the code loaded and a small amount of data pulled that allows you to test the training and
    prediction scripts. Inside the /test/integration-tests/ folder are a set of scripts that will test the code
    from end to end.
    <ol>
    <li>Run the training scripts inside /test/integration-tests/:
        <b>phase2_fc/test_phase2_train.py</b> and <b>phase2_gc/test_phase2_gc_train.py</b></li>
    <li>Run the prediction scripts inside /test/integration-tests/:
        <b>phase2_fc/test_phase2_train.py</b> and <b>phase2_gc/test_phase2_gc_train.py</b></li>
    </ol>
</div>

<div id="3">
<hr>
<h1>Configurations</h1>
The configurations are stored in YAML files inside <b>/data/yaml/</b>
<ol>
    <li>Train configurations are <b>fc_train.yml</b> for <b>ion3k</b> model and <b>gc_train.yml</b>
        for <b>global core</b> model.</li>
    <li>Prediction configurations are <b>fc_predict.yml</b> for <b>ion3k</b> model and <b>gc_predict.yml</b>
        for <b>global core</b> model.</li>
</ol>

    <u>Training parameters that are consistent for all domains (<b>fc_train.yml</b> and <b>gc_train.yml</b>)</u>
    <ul>
        <li>Preprocessing: master_df: filepaths</li>
        <ul>
            <li>normalizer_yml</li>
            <li>master_df_file</li>
            <li>remedy_associations_file</li>
        </ul>
        <li>Preprocessing: master_df: parameters</li>
        <ul>
            <li>user_label</li>
        </ul>
        <li>Preprocessing: master_df: fields</li>
        <ul>
            <li>normalized_fields</li>
        </ul>
        <li>Preprocessing: feature_df: filepaths</li>
        <ul>
            <li>train_master_data_dir</li>
            <li>train_pso_data_dir</li>
            <li>master_df_file</li>
            <li>timezone_lookup_file</li>
            <li>swarm_file</li>
            <li>catboost_encoder_file</li>
            <li>feature_df_file</li>
        </ul>
        <li>Preprocessing: feature_df: parameters</li>
        <ul>
            <li>backward_rolling_windows</li>
            <li>backward_sequence_window</li>
            <li>backward_single_event_window</li>
            <li>forward_rolling_windows</li>
            <li>forward_sequence_window</li>
            <li>forward_single_event_window</li>
            <li>ts_target</li>
        </ul>
        <li>Preprocessing: feature_df: fields</li>
        <ul>
            <li>cat_features</li>
        </ul>
        <li>HyperTicketing: train_hyper_ticket: filepaths</li>
        <ul>
            <li>master_df_folder_dir</li>
            <li>master_df_name</li>
        </ul>
        <li>HyperTicketing: train_hyper_ticket: parameters</li>
        <ul>
            <li>eval_beta</li>
            <li>init_pos</li>
            <li>max_iteration</li>
            <li>min_max</li>
            <li>num_par</li>
            <li>penalty</li>
            <li>pso_bound</li>
            <li>pso_params</li>
            <li>range_of_time</li>
        </ul>
        <li>SmartPriority: train_smart_priority: filepaths</li>
        <ul>
            <li>saved_swarm_folder_dir</li>
        </ul>
        <li>SmartPriority: train_smart_priority: parameters</li>
        <ul>
            <li>post_feature</li>
            <li>predict_var</li>
            <li>preprocess_method</li>
            <li>preprocess_nmax</li>
            <li>num_par</li>
            <li>preprocess_nmin</li>
        </ul>
        <li>Postprocessing: post_proc: filepaths</li>
        <ul>
            <li>classification_model_output_name</li>
            <li>master_file_name</li>
            <li>training_folder_dir</li>
        </ul>
        <li>Postprocessing: post_proc: parameters</li>
        <ul>
            <li>from_training</li>
            <li>rewrite</li>
            <li>streaming_to_index</li>
        </ul>
        <li>Postprocessing: post_proc: fields</li>
        <ul>
            <li>master_col</li>
            <li>output_col</li>
        </ul>
    </ul>

    <u>Training parameters that differ across domains.</u></br>For many of these, the only difference is a
        filepath with "fc" instead of "gc" or "ion3k" instead of "global_core".
    <ul>
        <li>Preprocessing: master_df: filepaths</li>
        <ul>
            <li>raw_data_dir</li>
            <li>save_data_dir</li>
            <li>device_lookup_files</li>
            <li>syslog_data_files</li>
            <li>remedy_inc_file</li>
            <li>remedy_labels_file</li>
            <li>user_label_dir</li>
            <li>dashboard_output_data_dir</li>
            <li>flaps_dashboard_file</li>
        </ul>
        <li>Preprocessing: master_df: parameters</li>
        <ul>
            <li>type</li>
            <li>remedy_window</li>
        </ul>
        <li>Preprocessing: master_df: fields</li>
        <ul>
            <li>master_df_fields (includes 'flap_count')</li>
        </ul>
        <li>Preprocessing: feature_df: filepaths</li>
        <ul>
            <li>raw_data_dir</li>
            <li>pso_dir</li>
            <li>data_dir</li>
        </ul>
        <li>Preprocessing: feature_df: parameters -- None</li>
        <ul>
        </ul>
        <li>Preprocessing: feature_df: fields</li>
        <ul>
            <li>num_feature (includes 'flap_count')</li>
        </ul>
        <li>HyperTicketing: train_hyper_ticket: filepaths</li>
        <ul>
            <li>saved_model_path</li>
        </ul>
        <li>HyperTicketing: train_hyper_ticket: parameters -- None</li>
        <ul>
        </ul>
        <li>SmartPriority: train_smart_priority: filepaths</li>
        <ul>
            <li>saved_swarm_dir</li>
            <li>save_output_dir</li>
        </ul>
        <li>SmartPriority: train_smart_priority: parameters</li>
        <ul>
            <li>classification_model_param_dict</li>
            <li>classification_threshold</li>
        </ul>
        <li>Postprocessing: post_proc: filepaths</li>
        <ul>
            <li>classification_pipeline_output_folder_dir</li>
            <li>prediction_folder_dir</li>
            <li>training_data_dir</li>
        </ul>
        <li>Postprocessing: post_proc: parameters -- None</li>
        <ul>
        </ul>
        <li>Postprocessing: post_proc: fields -- None</li>
        <ul>
        </ul>
    </ul>

The only additional differences between domains in the fc_predict.yml and gc_predict.yml files are changing "fc" to "gc" in the filepaths.
</div>


<div id="4">
<hr>
<h1>Input Files</h1>
<ol>
    <li>ION 3k
        <ol>
            <li>Syslog data <b>syslog_data_ion3k.csv</b> contains events data logged from devices </li>
            <li>Remedy Incident data <b>remedy_incident_data_ion3k.csv</b> contains the Remedy Incidents data</li>
            <li>Remedy Associations <b>remedy_associations.csv</b> contains the associations between different Remedy Incidents</li>
            <li>CMDB info <b>cmdb_info_ion3k.csv</b> contains info such as IP address, location, etc of the devices</li>
            <li>Feedback data <b>feedback_data_ion3k.csv</b> contains the events that have been given User feedback from
                project dashboard after being evaluated by experts and labelled thereafter as USEFUL or NOT USEFUL</li>
            <li>US cities location data <b>us-zip-code-latitude-and-longitude.csv</b> contains latitude, longitude,
                daylights saving time flag, etc. relevant info for each city in database</li>
        </ol>
    </li>
    <li>GLOBAL CORE
        <ol>
            <li>Syslog data <b>syslog_data_global_core.csv</b> contains events data logged from devices </li>
            <li>Remedy Incident data <b>remedy_incident_data_global_core</b> contains the Remedy Incidents data</li>
            <li>Remedy Associations <b>remedy_associations.csv</b> contains the associations between different Remedy
                Incidents</li>
            <li>CMDB info <b>cmdb_info_global_core.csv</b> contains info such as IP address, location, etc of the devices</li>
            <li>Flaps data for global core <b>flaps_dashboard.csv</b> contains the interface flaps data pulled from the
                dashboard for the global core data</li>
            <li>Feedback data <b>feedback_data_global_core.csv</b> contains the events that have been given User feedback from
                project dashboard after being evaluated by experts and labelled thereafter as USEFUL or NOT USEFUL</li>
            <li>US cities location data <b>us-zip-code-latitude-and-longitude.csv</b> contains latitude, longitude,
                daylights saving time flag, etc. relevant info for each city in database</li>
        </ol>
    </li>

</ol>
</div>

<hr>

<div id="5">
<h1>Model Pipeline Steps </h1>
<ol>
    <li>Preprocessing</li>
    <div>Incorporates all input data sources, merging the syslog data, remedy labels, remedy incidents and other data
        sources to create a master dataframe <b>master_df.csv</b></div>
    <li>Hyper Ticketing</li>
    <div>Runs the Particle Swarm Optimization algorithm to find related events and group them</div>
    <li>Feature Generation</li>
    <div>Collects the master dataframe and the Hyper Ticketing output, merges them, converts categorical fields to
        numeric fields and derives new fields such as rolling windows averages and then generates the <b>features_df.csv</b>
        features dataframe</div>
    <li>Smart Priority</li>
    <div>Takes in the features dataframe, normalizes the data to prepare for algorithm training, divides the data into training
    and testing sets, and then trains the classification algorithm on the training set.</div>
    <li>Postprocessing</li>
    <div>Saves all output to relevant directories</div>
</ol>
    <img src="model_pipeline.png" height="600px">
</div>

<hr>

<div id="6">
<h1>Output Files</h1>
<ol>
    <li>Training Output
        <ol>
            <li><b>master_df.csv</b> is the unified dataframe from all input sources</li>
            <li><b>feature_df.csv</b> contains the relevant features that are created as candidate features for input to
                the classification algorithm</li>
            <li><b>model_pipeline_run/</b>
                <ol>
                    <li><b>classification_model_file.pkl</b> is the pickelized classification model, created for further use</li>
                    <li><b>config_file.json</b> contains the configurations for training</li>
                    <li><b>model_output.csv</b> is the training data with predicted labels</li>
                    <li><b>scaler_model_file.pkl</b> is the pickelized version of the scaling model for the training/testing data</li>
                </ol>
            </li>
            <li><b>pso_filters_run/</b>
                <ol>
                    <li><b>pso_filter.json</b> is the configuration info for the Particle Swarm Optimization training</li>
                    <li><b>swarm_label.csv</b> is the output of the Particle Swarm Optimization, grouping similar tickets</li>
                </ol>
            </li>
            <li><b>catboost_encoder.pkl</b> is the pickelized model that converts categorical features to numerical features</li>
            <li><b>feature_importance.csv</b> contains the quantified effect each feature has on the algorithm</li>
        </ol>
    </li>
    <li>Prediction Output
        <ol>
            <li>
                <b>classification_online/online_prediction_run....</b>
                <ol>
                    <li><b>config_file.json</b> contains the configurations for prediction</li>
                    <li><b>online_prediction.csv</b> is the test data with predicted labels</li>
                </ol>
            </li>
            <li><b>dashboard/dashboard_output.csv</b> is the file that contains the final dashboard data pushed to Splunk</li>
            <li><b>pso_online/pso_online_filtering_run....</b>
                <ol>
                    <li><b>pso_online_filtering.json</b>  is the configuration info for the online Particle Swarm Optimization</li>
                    <li><b>swarm_label.csv</b>  is the output of the Particle Swarm Optimization, grouping similar tickets</li>
                </ol>
            </li>
            <li><b>feature_df.csv</b> contains the relevant features that are created as candidate features for input to
                the classification algorithm</li>
            <li><b>master_df.csv</b> is the unified dataframe from all input sources
            </li>
    </ol>
    </li>
</ol>
</div>

<hr>
<div id="7">
<h1>Expanding to a new domain</h1>
This section discusses the steps required to expand this solution to a new domain, outside of ion3k and global_core devices.

    <p>
    <h3>Preparation</h3>
    <ol>
    <li>Ensure that the information needed is available in splunk.
        <ul>
        <li>Device Syslog Messages (Including time stamp, severity, device name, and message)
        <li>CMDB Info for the new devices
        <li>Historical Remedy Ticket information for the new devices
        </ul>
    </li>
    <li>Acquire a list of the devices. Create a splunk query that builds a lookup table for the device names.
       For examples, see /data/lookups/.</li>
    </ol>
    </p>

    <p>
    <h3>Set up splunk pulls for the new device type </h3>
            <ol>
                <li>Set up configurations for syslog, CMDB info, Remedy Ticket information and user feedback data.
                    <ul>
                        <li>For Syslog, edit <b>input_data_config.yml</b> and add a key "syslog_data_&#60;device_type&#62;" with configurations:
                            <ul>
                                <li>url: The Splunk url to pull data from</li>
                                <li>url_token</li>
                                <li>url2</li>
                                <li>index: The Splunk query for the data</li>
                                <li>lookup: If the splunk query has any lookup files involved, add a lookup as a sub-query.
                                    For example, if the device list is stored as a csv on splunk,
                                    it can be referenced using this lookup</li>
                                <li>fields: All the fields from the table that are needed</li>
                            </ul>
                        </li>
                        <li>For CMDB Info, edit <b>input_data_config.yml</b> and under <b>cmdb_info</b> -> <b>lookups</b>,
                            add a key for <b>&#60;device_type&#62;</b>, under which add the Splunk sub-query that filters all
                            device names for the device type and rename the field as "host".<br>
                            See example -> <code>| search [| inputlookup cloudgenix_ion3k_lookup.csv| eval host=lower(CI_NAME) | table host] | eval host=upper(host)</code></li>

                        <li>For Remedy Tickets, edit <b>input_data_config.yml</b> and under <b>remedy_incident_data</b> -> <b>lookups</b>,
                            add a key for <b>&#60;device_type&#62;</b>, under which add the Splunk sub-query that filters all
                            device names for the device type and rename the field as "HPD_CI".<br>
                            See example -> <code>[|inputlookup cloudgenix_ion3k_lookup.csv | rename CI_NAME as HPD_CI] </code></li>

                        <li>For feedback data,edit <b>input_data_config.yml</b> and add a key "feedback_data_&#60;device_type&#62;" with configurations:
                            <ul>
                                <li>url: The Splunk url to pull data from</li>
                                <li>index: The Splunk query for the data</li>
                                <li>fields: All the fields from the table that are needed</li>
                            </ul>
                        </li>
                    </ul>
                </li>

                <li>Create directories in project structure to store the data for training and prediction
                    <ul>
                        <li> Function <b>build_directory_structure</b> located in <b>/run/get_training_data</b> is configured
                        to create directories for <b>fc</b> and <b>gc</b></li>
                        <li> Edit <b>build_directory_structure</b> line 4 which is   <br><code>for datatype in ("fc", "gc"):</code>
                            to   <br><code>for datatype in ("fc", "gc",<b>"&#60;device_type&#62;"</b>):</code></li>
                        <li> Calling this function (separately, or directly running the <b>get_training_data</b> script)
                        will create a directory structure which is:
                            <ul>
                                <li>data
                                    <ul>
                                        <li>fc</li>
                                        <li>gc</li>
                                        <i>
                                        <li><b>&#60;device_type&#62;</b>
                                            <ul>
                                                <li>predict_input
                                                    <ul>
                                                        <li>latest</li>
                                                        <li>phase2</li>
                                                    </ul>
                                                </li>
                                                <li>predict_output
                                                    <ul>
                                                        <li>prediction
                                                            <ul>
                                                                <li>dashboard</li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                                <li>train_input
                                                    <ul>
                                                        <li>phase2</li>
                                                    </ul>
                                                </li>
                                                <li>train_output</li>
                                            </ul>
                                        </li></i>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>

                <li>Add code to pull Splunk data in for training and prediction
                    <ol>
                        <li> In <b>run/get_training_data</b> add a function <code>pull_&#60;device_type&#62;(days=14, prediction_data=False)</code>
                            that calls the other functions that pull the data into local. <br>
                            See example -> <code>pull_ion3k(days=14, prediction_data=False)</code>.</li>
                        <li> In <b>run/train</b>, in function <code>pull_latest_data_for_training</code>,<br> edit line
                            <code>for device_type in ("ion3k", "global_core")</code><br> to <code>
                                for device_type in ("ion3k", "global_core","&#60;device_type&#62;")</code></li>
                        <li> In <b>run/predict</b>, in function <code>pull_latest_data_for_prediction</code>,<br> edit line
                            <code>for device_type in ("ion3k", "global_core")</code><br> to <code>
                                for device_type in ("ion3k", "global_core","&#60;device_type&#62;")</code></li>
                        <li> Make sure that the credentials being set for each device type and each data source is being set correctly, using the
                            function <code>sg_splunk.set_credentials()</code></li>
                    </ol>

                    Running get_training_data should put the data in the respective folders in <b>data/&#60;device_type&#62;</b>. The functions for
                    pulling data during <b>run/train</b> and <b>run/predict</b> have also been configured.
                </li>


            </ol>
    </p>

    <p>
        <h3>Changes to be made for training the model on the new device type</h3>
        <ul>
            <li>Configurations
                <ul>
                    <li>Create configuration file <b>&#60;device_type&#62;_train.yml</b> within the folder <b>/data/yaml</b></li>
                    <li> Copy contents from <b>/data/yaml/fc_train.yml</b> and replace <i>ion3k</i> with <i>&#60;device_type&#62;</i>
                        and <i>/fc</i> with <i>/&#60;device_type&#62;</i></li>
                </ul>
            </li>
            <li>In <b>src/preprocessing/SyslogNormalizer.py</b>, add function <code>normalize_&#60;device_type&#62;(self, &#60;device_type&#62;_df)</code>.<br>
                See example <code>normalize_ion3k(self, ion3k_df)</code>. Replace <i>ion3k</i> with <i>&#60;device_type&#62;</i> and <i>ion3k_df</i> with
                <i>&#60;device_type&#62;_df</i></li>
            <li>In <b>src/preprocessing/RawData.py</b>, go to line <br> <code> if datatype == "ion3k": <br>
            &emsp;&emsp;df = normalizer.normalize_ion3k(event_df)</code> <br> and append the case of <i>&#60;device_type&#62;</i>, by adding
            <br> <code> if datatype == "&#60;device_type&#62;": <br>
                &emsp;&emsp;df = normalizer.normalize_&#60;device_type&#62;(event_df)</code></li>
            <li>In <b>run/train</b>, within the function <code>run_train</code> add the code block <br>
                <code>    if device_type == "&#60;device_type&#62;": <br>
                        &emsp;&emsp;yaml_file = base_path + 'data/yaml/&#60;device_type&#62;_train.yml'</code></li>
            <li>In <b>run/train</b>, add the line <code>run_train(device_type="&#60;device_type&#62;")</code>
                in the <code>if __name__ == "__main__":</code> block</li>
        </ul>
        The train script will work for the new device type now.
    </p>

    <p>
        <h3>Changes to be made for running predictions on the trained model for the new device</h3>
        <ul>
            <li>Configurations
                <ul>
                    <li>Create configuration file <b>&#60;device_type&#62;_predict.yml</b> within the folder <b>/data/yaml</b></li>
                    <li> Copy contents from <b>/data/yaml/fc_predict.yml</b> and replace <i>ion3k</i> with <i>&#60;device_type&#62;</i>
                        and <i>/fc</i> with <i>/&#60;device_type&#62;</i></li>
                </ul>
            </li>
            <li>In <b>run/predict</b>, within the function <code>run_prediction</code> add the code block <br>
                <code>    if device_type == "&#60;device_type&#62;": <br>
                        &emsp;&emsp;yaml_file = base_path + 'data/yaml/&#60;device_type&#62;_predict.yml'</code></li>
            <li>In <b>run/predict</b>, add the line <code>run_prediction(device_type="&#60;device_type&#62;"current_time=current_time, minutes_cut_off=10)</code>
                in the <code>if __name__ == "__main__":</code> block</li>
        </ul>
    </p>

    <p>
        <h3>Changes to be made for retraining the model (Scheduling retrain jobs)</h3>
        <ul>
            <li>In <b>run/retrain</b>, within the function <code>run_train</code> add the code block <br>
                <code>    if device_type == "&#60;device_type&#62;": <br>
                        &emsp;&emsp;yaml_file = base_path + 'data/yaml/&#60;device_type&#62;_train.yml'<br>
                        &emsp;&emsp;now = datetime.now().strftime('%Y-%m-%d_%H%M%S' + "/")<br>
                        &emsp;&emsp;master_df_folder = base_path + "data/&#60;device_type&#62;/train_output/retrain/" + now <br>
                        &emsp;&emsp;os.makedirs(master_df_folder, exist_ok=True)<br>
                        &emsp;&emsp;pull_previous_master_df(master_df_folder, '&#60;device_type&#62;')</code></li>
            <li>In <b>run/retrain</b>, add the line <code>run_train(device_type="&#60;device_type&#62;)</code>
                in the <code>if __name__ == "__main__":</code> block</li>
        </ul>
    </p>

    <p>
        <h3>Changes to be made to run daily prediction jobs</h3>
        <ul>
            <li>In <b>run/predict_daily</b>, within the function <code>run_train</code> add the code block <br>
                <code>    if device_type == "&#60;device_type&#62;": <br>
                        &emsp;&emsp;yaml_file = base_path + 'data/yaml/&#60;device_type&#62;_predict.yml'<br>
                        </code></li>
            <li>In <b>run/predict_daily</b>, add the line <code>run_prediction(device_type="&#60;device_type&#62;, current_time, 60*24)</code>
                in the <code>if __name__ == "__main__":</code> block</li>
        </ul>
    </p>




</div>

<div id="8">
    <h1>Switching to a Production Environment</h1>

    <p><h3>Setting up credentials</h3>
        <ul>
            <li>Generating encryption key
                <ul>
                    <li>To use existing credentials, collect <b>smartgrid.key</b> and store it in <b>/data/keys</b></li>
                    <li>To generate a new key, run the function <code>generate_key()</code> from <b>/src/utils/encrypt.py</b>
                        which automatically creates and stores an encryption key in <b>/data/keys/smartgrid.key</b>.
                        The function can be edited to change the storage path of the key</li>
                </ul>
            <li>Encryption of credentials
                <ul>
                    <li>Credentials for production environment can be encrypted using the function <code>encrypt_message()</code>
                        from <b>/src/utils/encrypt.py</b> and stored in <b>/data/yaml/credentials.yml</b> </li>
                    <li>Encrypted credentials are read using <code>set_credentials()</code> in <b>/src/sg_splunk.py</b> and decrypted by calling
                        <code>decrypt_message()</code> from <b>/src/utils/encrypt.py</b> automatically</li>
                </ul>
            </li>
        </ul>
    </p>
</div>

<div id="9">
    <h1>Dashboard documentation</h1>

    <p>The documentation for the
        <a href="http://ah-1064594-001.sdi.corp.bankof.com:8000/en-US/app/search/smart_grid__financial_center_uat">
            Model Output dashboard for Financial Centre</a>
        and the
        <a href="http://ah-1064594-001.sdi.corp.bankof.com:8000/en-US/app/search/smart_grid__global_core_uat?form.Useful_pred=*&form.user_label=*&form.time_period.earliest=-7d%40h&form.time_period.latest=now&form.user_reason=None">
            Model Output dashboard for Global Core</a>
        is available
        <a href="http://wiki.bankof.com/display/NDNASASG/Smartgrid+Dashboard+Documentation">here</a>

    </p>
</div>

<hr>
<div id="10">
<h1>Takeaways</h1>
This section has some closing thoughts and takeaways from the development of the models.
</div>

</body>
</html>